{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286398ad-0c31-4eb7-8e26-a4a2b2466101",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each.\n",
    "\n",
    "\n",
    "\n",
    "Simple Linear Regression:\n",
    "\n",
    "Simple linear regression is a statistical method used to model the relationship between two variables: a dependent variable (also called the target or response variable) and an independent variable (also called the predictor variable). It assumes that there is a linear relationship between these two variables. The goal of simple linear regression is to find the best-fitting straight line (linear equation) that minimizes the sum of squared differences between the observed values of the dependent variable and the values predicted by the linear equation.\n",
    "\n",
    "Example of Simple Linear Regression:\n",
    "\n",
    "Let's say we want to predict a student's final exam score (dependent variable) based on the number of hours they studied (independent variable). We collect data from several students and plot the data points on a graph. Simple linear regression aims to find a line that best fits these data points and can be expressed as:\n",
    "\n",
    "Final Exam Score = Intercept + (Slope * Hours Studied) + Error\n",
    "\n",
    "Where the intercept and slope are the parameters of the linear equation that are estimated during the regression process, and the error accounts for the variability not explained by the model.\n",
    "\n",
    "Multiple Linear Regression:\n",
    "\n",
    "Multiple linear regression extends the concept of simple linear regression to involve multiple independent variables. Instead of just one predictor variable, it considers multiple predictor variables to model the relationship with the dependent variable. The equation for multiple linear regression becomes a linear combination of the predictor variables, each weighted by its respective coefficient. The goal is to find the best-fitting linear equation that explains the relationship between the dependent variable and multiple independent variables.\n",
    "\n",
    "Example of Multiple Linear Regression:\n",
    "\n",
    "Let's expand the previous example. In addition to the hours studied, we now also consider the number of prep tests taken by the student as a predictor. The multiple linear regression equation might look like this:\n",
    "\n",
    "Final Exam Score = Intercept + (Coefficient1 * Hours Studied) + (Coefficient2 * Number of Prep Tests) + Error\n",
    "\n",
    "Here, we have two predictor variables, each with its own coefficient. The intercept and coefficients are estimated through the regression process. The model aims to capture how both hours studied and the number of prep tests influence the final exam score.\n",
    "\n",
    "In summary, the key difference between simple linear regression and multiple linear regression is the number of independent variables being used to predict the dependent variable. Simple linear regression involves one independent variable, while multiple linear regression involves two or more independent variables.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?\n",
    "\n",
    "\n",
    "\n",
    "Linear regression comes with several assumptions that need to be satisfied for the model to be valid and for the results to be reliable. These assumptions are important because violating them can lead to biased or unreliable estimates. Here are the main assumptions of linear regression:\n",
    "\n",
    "Linearity: The relationship between the independent variables and the dependent variable should be linear. This means that the change in the dependent variable should be proportional to the change in the independent variables.\n",
    "\n",
    "Independence: The residuals (the differences between observed and predicted values) should be independent of each other. This assumption implies that the errors for one observation should not provide information about the errors for other observations.\n",
    "\n",
    "Homoscedasticity: Also known as constant variance, this assumption states that the variability of the residuals should remain constant across all levels of the independent variables. In other words, the spread of the residuals should be roughly the same throughout the range of predicted values.\n",
    "\n",
    "Normality of Residuals: The residuals should follow a normal distribution. This assumption is important for hypothesis testing and constructing confidence intervals.\n",
    "\n",
    "No or Little Multicollinearity: If you're using multiple independent variables, they should not be highly correlated with each other. High multicollinearity can make it difficult to determine the individual effect of each predictor variable on the dependent variable.\n",
    "\n",
    "To check whether these assumptions hold in a given dataset, you can use various diagnostic techniques:\n",
    "\n",
    "Residual Plot: Plot the residuals against the predicted values. If the residuals are randomly scattered around the horizontal line (zero), the linearity assumption might be satisfied. If the spread of residuals is consistent across the range of predicted values, homoscedasticity might be met.\n",
    "\n",
    "Normality Check: Create a histogram or a Q-Q plot of the residuals. If the residuals are approximately normally distributed, the normality assumption could hold. You can also use statistical tests like the Shapiro-Wilk test or the Anderson-Darling test to formally test for normality.\n",
    "\n",
    "Independence: Check for patterns in the residual plot that might suggest a lack of independence. Time series data might exhibit autocorrelation, which violates this assumption.\n",
    "\n",
    "Multicollinearity: Calculate the correlation matrix of the predictor variables. If you find high correlations (close to 1 or -1) between pairs of variables, it might indicate multicollinearity. Variance Inflation Factor (VIF) can also be calculated to quantify multicollinearity.\n",
    "\n",
    "Cook's Distance: This measures the influence of each data point on the regression coefficients. Large Cook's distances might indicate influential outliers.\n",
    "\n",
    "Durbin-Watson Test: For time series data, this test can help check for autocorrelation in the residuals.\n",
    "\n",
    "White's Test for Heteroscedasticity: This test can formally check for homoscedasticity by assessing whether the residuals' variance is consistent across different levels of predicted values.\n",
    "\n",
    "It's important to note that no dataset will perfectly meet all assumptions, but a reasonable approximation is often sufficient. If assumptions are severely violated, you might need to explore more advanced regression techniques or consider transforming the data to meet the assumptions better.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario.\n",
    "\n",
    "\n",
    "In a linear regression model, the slope and intercept have specific interpretations that help us understand the relationship between the independent variable(s) and the dependent variable.\n",
    "\n",
    "Intercept (β₀): The intercept is the value of the dependent variable when all independent variables are zero. It represents the starting point of the regression line on the y-axis, where the line crosses the y-axis.\n",
    "\n",
    "Slope (β₁): The slope represents the change in the dependent variable for a one-unit change in the independent variable. It tells us how much the dependent variable is expected to change on average when the independent variable increases by one unit, holding other variables constant.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's consider a real-world scenario: predicting a person's monthly electricity bill based on the number of appliances they use.\n",
    "\n",
    "Linear Regression Equation:\n",
    "Electricity Bill = Intercept + (Slope * Number of Appliances) + Error\n",
    "\n",
    "Here, the intercept would represent the expected electricity bill when the person uses zero appliances. However, this interpretation might not make practical sense in this context, as even without using any appliances, there would still be a baseline electricity cost.\n",
    "\n",
    "The slope, on the other hand, is more informative. Let's say the calculated slope is 20. This means that, on average, for each additional appliance a person uses, their monthly electricity bill is expected to increase by $20, assuming all other factors remain constant. This provides a clear quantitative understanding of the relationship between the number of appliances and the electricity bill.\n",
    "\n",
    "For instance, if a person currently uses 5 appliances and their monthly electricity bill is $100, a one-unit increase in the number of appliances (from 5 to 6) would be associated with an expected increase in the bill of $20. So, with 6 appliances, their bill would be expected to be around $120.\n",
    "\n",
    "It's important to note that these interpretations assume the linear relationship holds and that the assumptions of linear regression are met. Additionally, if you have multiple predictor variables in a multiple linear regression, the interpretation of the slope for one variable would be with other variables held constant.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "\n",
    "\n",
    "Gradient Descent:\n",
    "\n",
    "Gradient descent is an optimization technique used to minimize a function by iteratively adjusting the parameters of the function in the direction of steepest descent. In simpler terms, it's a method for finding the minimum of a function by taking steps proportional to the negative of the gradient (slope) of the function at the current point. The gradient provides the direction of the steepest increase in the function's value, so moving in the opposite direction reduces the function's value.\n",
    "\n",
    "How Gradient Descent is Used in Machine Learning:\n",
    "\n",
    "Gradient descent is a fundamental concept in machine learning, especially for training models with adjustable parameters. In machine learning, the goal is often to find the parameters of a model that minimize a specific cost function or error metric. This process is also known as model training or optimization.\n",
    "\n",
    "Here's how gradient descent is used in machine learning:\n",
    "\n",
    "Cost Function: In machine learning, you define a cost function that quantifies how well your model's predictions match the actual target values. The goal is to minimize this cost function.\n",
    "\n",
    "Parameter Initialization: You start with an initial set of parameter values for your model. These parameters define the relationship between the input data and the predictions.\n",
    "\n",
    "Iteration: The algorithm iteratively updates the model's parameters using gradient descent. At each iteration, it calculates the gradient of the cost function with respect to the parameters. The gradient points in the direction of the steepest increase in the cost function.\n",
    "\n",
    "Update Parameters: The algorithm then adjusts the parameters by taking a step in the opposite direction of the gradient. The size of the step is determined by a parameter called the learning rate, which controls the step size in each iteration.\n",
    "\n",
    "Convergence: The algorithm repeats these iterations until a stopping criterion is met, which could be a specific number of iterations, reaching a certain level of convergence, or other criteria. Convergence occurs when the parameters no longer change significantly, indicating that the algorithm has found a local minimum of the cost function.\n",
    "\n",
    "Optimal Parameters: Once the algorithm converges, the final values of the parameters are considered the optimal values that minimize the cost function. These parameters define the trained model that can make accurate predictions on new data.\n",
    "\n",
    "Gradient descent comes in different variants, such as stochastic gradient descent (SGD), mini-batch gradient descent, and others, each with their own characteristics and trade-offs in terms of convergence speed and computational efficiency.\n",
    "\n",
    "Overall, gradient descent is a crucial technique in machine learning for training models and finding the optimal set of parameters that make the model perform well on the given task.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "\n",
    "\n",
    "Multiple Linear Regression Model:\n",
    "\n",
    "Multiple linear regression is an extension of simple linear regression that allows us to model the relationship between a dependent variable and multiple independent variables. In multiple linear regression, we assume that the dependent variable is a linear combination of the independent variables, each weighted by its own coefficient, plus an intercept term. Mathematically, the multiple linear regression equation can be represented as:\n",
    "\n",
    "�\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "�\n",
    "1\n",
    "+\n",
    "�\n",
    "2\n",
    "�\n",
    "2\n",
    "+\n",
    "…\n",
    "+\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " x \n",
    "1\n",
    "​\n",
    " +β \n",
    "2\n",
    "​\n",
    " x \n",
    "2\n",
    "​\n",
    " +…+β \n",
    "p\n",
    "​\n",
    " x \n",
    "p\n",
    "​\n",
    " +ε\n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "y is the dependent variable.\n",
    "�\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    "  is the intercept.\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    "…\n",
    ",\n",
    "�\n",
    "�\n",
    "β \n",
    "1\n",
    "​\n",
    " ,β \n",
    "2\n",
    "​\n",
    " ,…,β \n",
    "p\n",
    "​\n",
    "  are the coefficients corresponding to independent variables \n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    "…\n",
    ",\n",
    "�\n",
    "�\n",
    "x \n",
    "1\n",
    "​\n",
    " ,x \n",
    "2\n",
    "​\n",
    " ,…,x \n",
    "p\n",
    "​\n",
    " .\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    "…\n",
    ",\n",
    "�\n",
    "�\n",
    "x \n",
    "1\n",
    "​\n",
    " ,x \n",
    "2\n",
    "​\n",
    " ,…,x \n",
    "p\n",
    "​\n",
    "  are the independent variables.\n",
    "�\n",
    "ε represents the error term.\n",
    "The goal of multiple linear regression is to estimate the coefficients \n",
    "�\n",
    "0\n",
    ",\n",
    "�\n",
    "1\n",
    ",\n",
    "…\n",
    ",\n",
    "�\n",
    "�\n",
    "β \n",
    "0\n",
    "​\n",
    " ,β \n",
    "1\n",
    "​\n",
    " ,…,β \n",
    "p\n",
    "​\n",
    "  that best fit the observed data, minimizing the sum of squared differences between the actual and predicted values.\n",
    "\n",
    "Differences from Simple Linear Regression:\n",
    "\n",
    "Number of Independent Variables:\n",
    "\n",
    "In simple linear regression, there is only one independent variable (predictor).\n",
    "In multiple linear regression, there are two or more independent variables (predictors).\n",
    "Equation:\n",
    "\n",
    "Simple linear regression has a straightforward equation: \n",
    "�\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "�\n",
    "+\n",
    "�\n",
    "y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " x+ε.\n",
    "Multiple linear regression involves a more complex equation with multiple coefficients and corresponding independent variables.\n",
    "Relationship Complexity:\n",
    "\n",
    "Simple linear regression models a linear relationship between two variables.\n",
    "Multiple linear regression models a linear relationship among multiple variables. It can capture more complex interactions between the different predictors.\n",
    "Interpretation:\n",
    "\n",
    "In simple linear regression, the slope represents the change in the dependent variable for a unit change in the independent variable.\n",
    "In multiple linear regression, the interpretation of a specific coefficient involves holding all other predictors constant while assessing the effect of a one-unit change in the predictor associated with that coefficient.\n",
    "Visual Representation:\n",
    "\n",
    "Simple linear regression can be easily visualized with a scatter plot and a single regression line.\n",
    "Multiple linear regression's visualization becomes more complex, often requiring advanced techniques like scatterplot matrices or partial regression plots.\n",
    "Challenges:\n",
    "\n",
    "Multiple linear regression can be more challenging to interpret and analyze due to the increased number of predictors and potential multicollinearity issues.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?\n",
    "\n",
    "\n",
    "Multicollinearity in Multiple Linear Regression:\n",
    "\n",
    "Multicollinearity is a phenomenon that occurs in multiple linear regression when two or more independent variables in the model are highly correlated with each other. It can cause issues because it makes it challenging to determine the individual effect of each independent variable on the dependent variable. When multicollinearity is present, the coefficients of the correlated variables become unstable and can lead to misleading or counterintuitive interpretations.\n",
    "\n",
    "High multicollinearity can cause the following problems:\n",
    "\n",
    "Unreliable Coefficients: The coefficients of the correlated variables can become extremely sensitive to small changes in the data. This makes it difficult to determine the true effect of each variable.\n",
    "\n",
    "Inflated Standard Errors: Multicollinearity can lead to larger standard errors of the coefficients, making it harder to detect statistically significant relationships.\n",
    "\n",
    "Decreased Interpretability: It becomes challenging to interpret the impact of individual variables on the dependent variable due to the confounding effects caused by correlation.\n",
    "\n",
    "Detecting Multicollinearity:\n",
    "\n",
    "There are several ways to detect multicollinearity:\n",
    "\n",
    "Correlation Matrix: Calculate the correlation coefficients between pairs of independent variables. High correlation coefficients (close to 1 or -1) indicate potential multicollinearity.\n",
    "\n",
    "Variance Inflation Factor (VIF): VIF quantifies the extent to which the variance of a coefficient is increased due to multicollinearity. A VIF value greater than 5 or 10 is often considered indicative of multicollinearity.\n",
    "\n",
    "Eigenvalues of the Correlation Matrix: Calculate the eigenvalues of the correlation matrix. If there are small eigenvalues (close to zero), it suggests multicollinearity.\n",
    "\n",
    "Addressing Multicollinearity:\n",
    "\n",
    "If multicollinearity is detected, there are several strategies to address the issue:\n",
    "\n",
    "Feature Selection: Remove one or more of the correlated variables from the model. Choose the variable that is less theoretically relevant or has weaker statistical significance.\n",
    "\n",
    "Combine Variables: Create a new variable that combines the information from the correlated variables. This can be done through mathematical transformations or domain-specific knowledge.\n",
    "\n",
    "Ridge Regression: Ridge regression (a type of regularization) adds a penalty term to the loss function, which can mitigate the impact of multicollinearity on coefficient estimates.\n",
    "\n",
    "Principal Component Analysis (PCA): PCA transforms the original variables into a new set of uncorrelated variables (principal components). These components can be used in the regression instead of the original variables.\n",
    "\n",
    "Collect More Data: Increasing the sample size can sometimes reduce the impact of multicollinearity.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Polynomial Regression Model:\n",
    "\n",
    "Polynomial regression is a type of regression analysis where the relationship between the independent variable(s) and the dependent variable is modeled as an nth-degree polynomial. In simple terms, it's an extension of linear regression that allows you to fit a curved line to the data instead of a straight line. The equation for polynomial regression can be represented as:\n",
    "\n",
    "�\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "�\n",
    "+\n",
    "�\n",
    "2\n",
    "�\n",
    "2\n",
    "+\n",
    "�\n",
    "3\n",
    "�\n",
    "3\n",
    "+\n",
    "…\n",
    "+\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " x+β \n",
    "2\n",
    "​\n",
    " x \n",
    "2\n",
    " +β \n",
    "3\n",
    "​\n",
    " x \n",
    "3\n",
    " +…+β \n",
    "n\n",
    "​\n",
    " x \n",
    "n\n",
    " +ε\n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "y is the dependent variable.\n",
    "�\n",
    "x is the independent variable.\n",
    "�\n",
    "0\n",
    ",\n",
    "�\n",
    "1\n",
    ",\n",
    "…\n",
    ",\n",
    "�\n",
    "�\n",
    "β \n",
    "0\n",
    "​\n",
    " ,β \n",
    "1\n",
    "​\n",
    " ,…,β \n",
    "n\n",
    "​\n",
    "  are the coefficients of the polynomial terms.\n",
    "�\n",
    "ε represents the error term.\n",
    "By increasing the degree \n",
    "�\n",
    "n, you can fit curves of higher complexity to the data. Polynomial regression can capture more intricate relationships that linear regression might not be able to accommodate.\n",
    "\n",
    "Differences from Linear Regression:\n",
    "\n",
    "Equation Complexity:\n",
    "\n",
    "Linear regression involves a simple equation (\n",
    "�\n",
    "=\n",
    "�\n",
    "0\n",
    "+\n",
    "�\n",
    "1\n",
    "�\n",
    "+\n",
    "�\n",
    "y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " x+ε) that represents a straight line relationship between variables.\n",
    "Polynomial regression uses a more complex equation with additional polynomial terms (\n",
    "�\n",
    "2\n",
    ",\n",
    "�\n",
    "3\n",
    ",\n",
    "…\n",
    "x \n",
    "2\n",
    " ,x \n",
    "3\n",
    " ,…) to model curved relationships.\n",
    "Fitting Curves:\n",
    "\n",
    "Linear regression fits a straight line to the data, which might not adequately capture non-linear relationships.\n",
    "Polynomial regression can fit curves of various shapes, allowing it to capture non-linear patterns in the data.\n",
    "Model Flexibility:\n",
    "\n",
    "Linear regression is restricted to linear relationships between variables.\n",
    "Polynomial regression is more flexible and can capture non-linear relationships by using higher-degree polynomial terms.\n",
    "Interpretation:\n",
    "\n",
    "In linear regression, the interpretation of coefficients is relatively straightforward. The coefficient \n",
    "�\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    "  represents the change in the dependent variable for a one-unit change in the independent variable.\n",
    "In polynomial regression, the interpretation becomes more complex, as each coefficient is associated with a polynomial term and its effect on the dependent variable depends on the specific degree and value of the term.\n",
    "Overfitting:\n",
    "\n",
    "Polynomial regression with high-degree polynomials can lead to overfitting, where the model fits the noise in the data rather than the underlying pattern. This can result in poor generalization to new data.\n",
    "Model Selection:\n",
    "\n",
    "Choosing the appropriate degree for the polynomial is essential. A too high degree can lead to overfitting, while a too low degree might not capture the actual relationships.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Advantages of Polynomial Regression:\n",
    "\n",
    "Capturing Non-Linearity: Polynomial regression can capture complex non-linear relationships that linear regression cannot. It's particularly useful when the true relationship between variables isn't well represented by a straight line.\n",
    "\n",
    "Flexibility: By using higher-degree polynomial terms, you can model a wide range of shapes and curves, providing a more flexible approach to fitting the data.\n",
    "\n",
    "Improved Fit: In cases where a linear model would result in a poor fit to the data, polynomial regression can provide a better fit, leading to improved predictive performance.\n",
    "\n",
    "Disadvantages of Polynomial Regression:\n",
    "\n",
    "Overfitting: Polynomial regression with high-degree polynomials can lead to overfitting, where the model captures noise in the data rather than the underlying pattern. This can result in poor generalization to new data.\n",
    "\n",
    "Complexity and Interpretation: As the degree of the polynomial increases, the model becomes more complex and difficult to interpret. Higher-degree polynomials can result in numerous coefficients that might not have clear or intuitive meanings.\n",
    "\n",
    "Increased Variability: As the degree of the polynomial increases, the model's predictions can become more erratic, especially beyond the range of the observed data.\n",
    "\n",
    "Situations for Using Polynomial Regression:\n",
    "\n",
    "You might prefer to use polynomial regression in the following situations:\n",
    "\n",
    "Curved Relationships: When you have reasons to believe that the true relationship between variables is not linear, polynomial regression can help capture the underlying non-linear pattern.\n",
    "\n",
    "Limited Alternative Models: If other regression techniques, such as logarithmic or exponential regression, don't fit the data well and polynomial relationships seem plausible, polynomial regression can be a viable option.\n",
    "\n",
    "Domain Knowledge: If you have domain-specific knowledge that suggests a polynomial relationship between variables, polynomial regression can help reflect that knowledge in the model.\n",
    "\n",
    "Visual Inspection: When visualizing the data shows a clear curvature that linear regression cannot capture, polynomial regression might be a suitable choice.\n",
    "\n",
    "Small Degree Polynomials: If you want to introduce a small degree polynomial term (e.g., quadratic) into a linear regression model, it can help capture subtle curvatures while avoiding excessive complexity.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
